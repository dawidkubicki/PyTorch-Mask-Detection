{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'winsound' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b544290c7492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mno_mask_count\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mflag_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfive_second_count\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mwinsound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mno_mask_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mfive_second_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'winsound' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as tt\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from threading import Thread\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "flag_check=20\n",
    "stats= ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "train_tfms = tt.Compose([tt.Resize((224, 224)),\n",
    "                         #tt.RandomHorizontalFlip(),\n",
    "                        tt.ToTensor(),\n",
    "                        tt.Normalize(*stats)\n",
    "                        ])\n",
    "\n",
    "\n",
    "\n",
    "class ConvNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,3,1)\n",
    "        self.conv2 = nn.Conv2d(6,16,3,1)\n",
    "        self.fc1 = nn.Linear(54*54*16, 220)\n",
    "        self.fc2 = nn.Linear(220, 100)\n",
    "        self.fc3 = nn.Linear(100,2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 54*54*16)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        \n",
    "        return F.log_softmax(X, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "device = torch.device(\"cpu\")\n",
    "detector = dlib.get_frontal_face_detector() #Face detector obtained from OpenCV's dlib (Deep Learning Library)\n",
    "\n",
    "no_mask_count = 0\n",
    "five_second_count = 0\n",
    "\n",
    "# def load_checkpoint(filepath):\n",
    "#     checkpoint = torch.load(filepath, device)\n",
    "#     model = checkpoint['model']\n",
    "#     model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "#     for parameter in model.parameters():\n",
    "#         parameter.requires_grad = False\n",
    "\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "filepath = '/Users/dawidkubicki/Documents/models/entire_model.pt'\n",
    "# loaded_model = load_checkpoint(filepath)\n",
    "\n",
    "\n",
    "model = torch.load(filepath, map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "\n",
    "#Opens the video camera, sends each frame of the video to the model and obtains the prediction.This goes on till the 'Esc' key is pressed.\n",
    "\n",
    "while True:\n",
    "    _, frame = video.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        x,y = face.left(), face.top()\n",
    "        x1,y1 = face.right(), face.bottom()\n",
    "\n",
    "        crop = frame[y:y1, x:x1]\n",
    "        pil_image = Image.fromarray(crop, mode=\"RGB\")\n",
    "        pil=train_tfms(pil_image)\n",
    "        img = pil.unsqueeze(0)\n",
    "\n",
    "        yb = model(img)\n",
    "        _, max  = torch.max(yb, dim=1)\n",
    "        prediction = max.item()\n",
    "\n",
    "#Puts a bounding box around the face with the corresponding text based on the prediction obtained\n",
    "\n",
    "        if prediction == 1:\n",
    "            cv2.putText(frame, \"Wearing a Mask\", (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,255,0),2)\n",
    "            cv2.rectangle(frame, (x,y)  , (x1,y1), (0,255,0), 2)\n",
    "            no_mask_count = 0\n",
    "\n",
    "        elif prediction == 0:\n",
    "            cv2.putText(frame, \"Not Wearing a Mask\", (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0),2)\n",
    "            cv2.rectangle(frame, (x,y), (x1,y1), (255,0,0), 2)\n",
    "            no_mask_count += 1\n",
    "\n",
    "            if no_mask_count >= flag_check:\n",
    "                if five_second_count==0:\n",
    "                    print(\"no mask\")\n",
    "                no_mask_count = 0\n",
    "                five_second_count += 1\n",
    "\n",
    "                if five_second_count > 2:\n",
    "                    print(\"no more mask\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
